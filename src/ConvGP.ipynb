{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from  scipy import ndimage\n",
    "import random\n",
    "from glob import glob\n",
    "import stgp\n",
    "from sklearn import neighbors, svm, tree, naive_bayes, ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "import search\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data for training/testing\n",
    "1. Read the images from disk\n",
    "2. Save these in a dict from label -> images\n",
    "3. Split these based off label into training/testing images\n",
    "\n",
    "Need to do it in this order to ensure we get an equal split of instances from each class in the data, since classification accuracy is used as fitness this is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads in all the data as a dict from label -> [images]\n",
    "def read_data(directory, scale=False, scaled_height=None, scaled_width=None):\n",
    "    data = {}\n",
    "\n",
    "    # Assumes the images are in subfolders, where the folder name is the images label\n",
    "    for subdir in glob(directory+\"/*/\"):\n",
    "        label = subdir.split(\"/\")[-2] # Second to last element is thee class/sub folder name\n",
    "        images = [ndimage.imread(image, flatten=True) for image in glob(subdir+\"/*.*\")] # Read in all the images from subdirectories. Flatten for greyscale\n",
    "    \n",
    "        images = [image.astype(float) / 255. for image in images] # Store in range 0..1 rather than 0..255\n",
    "        \n",
    "        if scale: # Resize images\n",
    "            images = [transform.resize(image, (scaled_height, scaled_width)) for image in images]\n",
    "            \n",
    "        # Shuffle the images (seed specified at the top of program so this will be reproducable)\n",
    "        random.shuffle(images)\n",
    "        data[label] = images\n",
    "        \n",
    "    # Set of all class names\n",
    "    class_names = list(data.keys())\n",
    "\n",
    "    # Sanity check\n",
    "    if len(class_names) != 2:\n",
    "        print(\"Binary classification only! But labels found were:\", labels)\n",
    "    \n",
    "    return data, class_names\n",
    "\n",
    "# Splits the data into four arrays trainingX, trainingY, testingX, testingY\n",
    "def format_and_split_data(data, class_names, split, seed):\n",
    "    trainingX = []\n",
    "    trainingY = []\n",
    "    \n",
    "    testingX = []\n",
    "    testingY = []\n",
    "    \n",
    "    # For all the classes, split into training/testing (need to do it per class to ensure we get a good split of all classes)\n",
    "    for label in class_names:\n",
    "        x = data[label]\n",
    "        length = int(len(x))\n",
    "        y = [label] * length\n",
    "        \n",
    "        training_length = int(length * split)\n",
    "        trainingX.extend(x[:training_length])\n",
    "        trainingY.extend(y[:training_length])\n",
    "        \n",
    "        testingX.extend(x[training_length:])\n",
    "        testingY.extend(y[training_length:])\n",
    "    \n",
    "    # And just so the order isnt all class1s then all class2s, shuffle the data in unison\n",
    "    trainingX, trainingY = shuffle(trainingX, trainingY, random_state=seed)\n",
    "    testingX, testingY = shuffle(testingX, testingY, random_state=seed)\n",
    "\n",
    "    return trainingX, trainingY, testingX, testingY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the various models\n",
    "\n",
    "Now we have the data, we can run and evaluate the various algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_float(f):\n",
    "    return \"{0:.2f}\".format(f)\n",
    "\n",
    "# The method of comparison\n",
    "def classification_accuracy(real_labels, predicted_labels):\n",
    "    return metrics.accuracy_score(real_labels, predicted_labels)\n",
    "\n",
    "def fit_and_evaluate(model, trainingX, trainingY, testingX, testingY, seed=None, verbose=False):\n",
    "    start = time.time() # Track the time taken\n",
    "\n",
    "    if seed is not None:\n",
    "        model.fit(trainingX, trainingY, seed=seed, verbose=verbose)\n",
    "    else:\n",
    "        model.fit(trainingX, trainingY)\n",
    "    \n",
    "    training_time = time.time() - start\n",
    "        \n",
    "    predicted_training = model.predict(trainingX)\n",
    "    \n",
    "    start = time.time()\n",
    "    predicted_testing = model.predict(testingX)\n",
    "    testing_time = time.time() - start\n",
    "    \n",
    "    return classification_accuracy(trainingY, predicted_training), classification_accuracy(testingY, predicted_testing), training_time, testing_time \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(title, arr):\n",
    "    print(title, pretty_float(np.min(arr)), pretty_float(np.mean(arr)), pretty_float(np.max(arr)), len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_general_classifiers(trainingX, trainingY, testingX, testingY):\n",
    "    print(\"General Classifiers\")\n",
    "\n",
    "    # The general classification methods require a list of features, rather than a 2d array so we need to flatten these\n",
    "    flattened_trainingX = [image.flatten() for image in trainingX]\n",
    "    flattened_testingX = [image.flatten() for image in testingX]\n",
    "\n",
    "    # The general classifiers to compare against\n",
    "    general_classifiers = {\n",
    "        \"Nearest Neighbour\": neighbors.KNeighborsClassifier(1),\n",
    "        \"SVM\": svm.SVC(),\n",
    "        \"Decision Tree\": tree.DecisionTreeClassifier(),\n",
    "        \"Naive Bayes\": naive_bayes.GaussianNB(),\n",
    "        \"Adaboost\": ensemble.AdaBoostClassifier()\n",
    "    }\n",
    "\n",
    "    print(\"Name, Training accuracy, Testing Accuracy, Training Time, Testing Time\")\n",
    "    # These methods are deterministic, so only need to be run once\n",
    "    for classifier in general_classifiers:\n",
    "        model = general_classifiers[classifier]\n",
    "        training_accuracy, testing_accuracy, training_time, testing_time = fit_and_evaluate(model, flattened_trainingX, trainingY, flattened_testingX, testingY)\n",
    "        print(classifier, pretty_float(training_accuracy * 100), pretty_float(testing_accuracy * 100), pretty_float(training_time * 1000), pretty_float(testing_time * 1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_convgp(trainingX, trainingY, testingX, testingY, num_runs, gd_frequency):\n",
    "    print(\"ConvGP\")\n",
    "    \n",
    "    convgp = stgp.ConvGP(generations=1, pop_size=5, gd_frequency=gd_frequency)\n",
    "    \n",
    "    if gd_frequency == -1:\n",
    "        print(\"Without gradient descent\")\n",
    "    else:\n",
    "        print(\"With gradient descent every\", gd_frequency, \"generations\")\n",
    "        \n",
    "    # Since GP has an element of stochasticity, need to run the evolutionary process several times with different evolutionary seeds\n",
    "    seeds = random.sample(range(num_runs), num_runs) # Random numbers from 0..num_runs, range of values doesnt really matter as long as we have num_runs of them\n",
    "\n",
    "    # Store the results over all runs\n",
    "    training_accuracies = []\n",
    "    testing_accuracies = []\n",
    "    training_times = []\n",
    "    testing_times = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        training_accuracy, testing_accuracy, training_time, testing_time = fit_and_evaluate(convgp, trainingX, trainingY, testingX, testingY, seed=seed, verbose=False)\n",
    "\n",
    "        training_accuracies.append(training_accuracy * 100) # As a percentage\n",
    "        testing_accuracies.append(testing_accuracy * 100)\n",
    "        training_times.append(training_time * 1000) # As millis\n",
    "        testing_times.append(testing_time * 1000)\n",
    "\n",
    "    print(\"Measure:\", \"Min\", \"Mean\", \"Max\", \"N\")\n",
    "    print_stats(\"Training Accuracy:\", training_accuracies)\n",
    "    print_stats(\"Testing Accuracy:\", testing_accuracies)\n",
    "    print_stats(\"Training Time:\", training_times)\n",
    "    print_stats(\"Testing Time:\", testing_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(dataset_name, seed, scale=False, training_split=0.5):\n",
    "    # Reproducability\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Which data to use\n",
    "    data_directory = \"data/\"\n",
    "\n",
    "    print(\"Data is:\", dataset_name)\n",
    "    print(\"Seed for data shuffle is:\", seed)\n",
    "\n",
    "    # How many times to run the evolutionary process\n",
    "    num_runs = 30\n",
    "\n",
    "    # Used only if scale is set to True. Must be used if images are of different sizes\n",
    "    scaled_width = 256 \n",
    "    scaled_height = 256\n",
    "\n",
    "    # Read and split data into training and testing    \n",
    "    data, class_names = read_data(data_directory+dataset_name, scale, scaled_width, scaled_height)\n",
    "    trainingX, trainingY, testingX, testingY = format_and_split_data(data, class_names, training_split, seed)\n",
    "    \n",
    "    run_general_classifiers(trainingX, trainingY, testingX, testingY)\n",
    "    run_convgp(trainingX, trainingY, testingX, testingY, num_runs, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset_name = sys.argv[0]\n",
    "    seed = sys.argv[1]\n",
    "    scale = True\n",
    "    \n",
    "    run(dataset_name, seed, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
